{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 of serious processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter\n",
    "* import previous database\n",
    "* split to sentence or paragraph level\n",
    "* compute sentiment with sentimentpl\n",
    "* how to train on own data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import newspaper\n",
    "from selenium import webdriver\n",
    "from sentimentpl.models import SentimentPLModel\n",
    "import spacy\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "model = SentimentPLModel(from_pretrained='latest')\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"~\\\\documents\\\\scraping_NLP\\\\df_corona.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pl_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentpl.models import SentimentPLModel\n",
    "\n",
    "model = SentimentPLModel(from_pretrained='latest')\n",
    "print(model('Jestem wesoły Romek').item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Heja, to ja. Lubię naleśnik. 10 tys. zakażonych ludzi czeka na szczepienie.\")\n",
    "doc = nlp(df['Clean_text'][0])\n",
    "for sent in doc.split:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = df['Clean_text'][0]\n",
    "good_split = []\n",
    "for i in range(len(article.split('.'))):\n",
    "    if article.split('.')[i][0].islower():\n",
    "        article.split('.')[i-1] = article.split('.')[i-1] + article.split('.')[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_split(text):\n",
    "    text = re.findall(\"[A-Z].*?[.!?]  \", text, re.MULTILINE | re.DOTALL )\n",
    "    senti = []\n",
    "    for s in text:\n",
    "        senti = senti + [(model(s).item())]\n",
    "    return(np.mean(senti))\n",
    "        \n",
    "    \n",
    "df['Clean_text']= df['Clean_text'].astype(str)\n",
    "    \n",
    "sents = df['Clean_text'][:100].map(sent_split)\n",
    "\n",
    "# sent_split(df['Clean_text'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentpl.models import SentimentPLModel\n",
    "\n",
    "model = SentimentPLModel(from_pretrained='latest')\n",
    "print(model('Jestem wesoły Romek').item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zbiorcze = []\n",
    "for s in sents[:10]:\n",
    "    senti = []\n",
    "    for para in s:\n",
    "        senti = senti + [(model(para).item())]\n",
    "    print(np.mean(senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sents)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sentimentpl.models import SentimentPLModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"~\\\\documents\\\\scraping_NLP\\\\df_corona.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polska_100 = df[df['Location'] == 'Polska'][:100]\n",
    "polska_100['Clean_text']= polska_100['Clean_text'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=polska_100['Clean_text'][443]\n",
    "re.findall(\"[A-Z].*?[.!?]\", dd, re.MULTILINE | re.DOTALL )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_split(text):\n",
    "    text = re.findall(\"[A-Z].*?[.!?]  \", text, re.MULTILINE | re.DOTALL )\n",
    "    senti = []\n",
    "    for s in text:\n",
    "        senti = senti + [(model(s).item())]\n",
    "    return(np.mean(senti))\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "\n",
    "splitter = SentenceSplitter(language='pl')\n",
    "trash = \"–\"\n",
    "dd= re.sub(trash, \"\", dd)\n",
    "# splitter.split(text=dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_split_2(text):\n",
    "    print('running')\n",
    "    splitter = SentenceSplitter(language='pl')\n",
    "    trash = \"–\"\n",
    "    text = re.sub(trash, \"\", text)\n",
    "    text = splitter.split(text=text)\n",
    "    senti = []\n",
    "    for s in text:\n",
    "        senti = senti + [(model(s).item())]\n",
    "    \n",
    "    return np.mean(senti)\n",
    "\n",
    "polska_100['sentiment'] = polska_100['Clean_text'].map(sent_split_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polska_100['sentiment'] = polska_100['Clean_text'].map(sent_split)\n",
    "min(polska_100['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "najgorsze = polska_100[polska_100['sentiment'] < -0.2]\n",
    "najlepsze =  polska_100[polska_100['sentiment'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "najlepsze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model('niemców stać na wszystko niemcy się bogacą niemiec potrafi').item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (model('niemców stać na wszystko').item())\n",
    "b = (model('niemcy się bogacą').item())\n",
    "c = (model('niemiec potrafi').item())\n",
    "a\n",
    "b\n",
    "c\n",
    "np.mean([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model('udało nam się bez problemu pogrześć mowę').item())\n",
    "(model('rzuciliśmy bez najmniejszego kłopotu ziemię skąd nasz ród').item())\n",
    "(model('dysydenci zostali bardzo sprawnie zesłani na Syberię').item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_100 = pd.to_datetime(df['Date'][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates_100, sents)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(polska_100['sentiment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "from nltk.corpus import stopwords\n",
    "filtered_words = [word.lower() for word in word_list if word not in stopwords.words('polish')]\n",
    "corpus = polska_100['Clean_text']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "clean_big = []\n",
    "for doc in corpus:\n",
    "    clean = []\n",
    "    for word in word_tokenize(doc):\n",
    "        if word.lower() not in stopwords.words('english'):\n",
    "            print(word)\n",
    "            clean.append(word)\n",
    "    print(clean)\n",
    "    clean_big += clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    for word in word_tokenize(doc):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plwordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corona.to_excel(\"corona_gucci_20210523.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_split_2(text):\n",
    "    print('running')\n",
    "    splitter = SentenceSplitter(language='pl')\n",
    "    trash = \"–\"\n",
    "    text = re.sub(trash, \"\", text)\n",
    "    text = splitter.split(text=text)\n",
    "    senti = []\n",
    "    for s in text:\n",
    "        senti = senti + [(model(s).item())]\n",
    "    \n",
    "    return np.mean(senti)\n",
    "\n",
    "polska_100['sentiment'] = polska_100['Clean_text'].map(sent_split_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corona = df_corona.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_corona['Clean_text'][1]\n",
    "splitter = SentenceSplitter(language = 'pl')\n",
    "trash = \"–\"\n",
    "text = re.sub(trash, \"\", text)\n",
    "text = splitter.split(text=text)\n",
    "def sentiment_sentence(sent):\n",
    "    return (model(sent).item())\n",
    "def sentiment_article(article):\n",
    "    if random.randint(1,10) > 5:\n",
    "        print(\"surprise! still working\")\n",
    "    trash = \"–\"\n",
    "    article = re.sub(trash, \"\", article)\n",
    "    article = splitter.split(text=article)\n",
    "    return np.mean(list(map(sentiment_sentence, article)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corona = pd.read_excel(\"corona_gucci_20210523.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "all_sentiment = df_corona['Clean_text'].map(sentiment_article)\n",
    "end = time.process_time()\n",
    "elapsed = start - end\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corona['Mean_sentiment'] = all_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/bieli/stopwords/master/polish.stopwords.txt'\n",
    "page = requests.get(url)\n",
    "stopwords = set((page.text).split(\"\\n\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ' '.join(df_corona['Clean_text']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = [w for w in full_text.split() if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(split_text).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.pl.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
