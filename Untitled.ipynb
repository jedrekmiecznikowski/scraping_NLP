{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping TVP INFO\n",
    "\n",
    "gonna start with beautifulsoup and requests like a proper christian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = requests.get(\"https://www.tvp.info/\")\n",
    "coverpage = r1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(coverpage, 'html5lib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('.*news__content*')\n",
    "\n",
    "coverpage_news = soup1.find_all(\"div\", {\"class\" : regex})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverpage_news[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverpage_news[0].find('a')['href'] #url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverpage_news[3].find('h3').get_text() #title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "urls = [coverpage_news[i].find('a')['href'] for i in range(len(coverpage_news))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "regex_h = re.compile('[h1|h2|h3]')\n",
    "for i in range(len(coverpage_news)):\n",
    "    try:\n",
    "        t = coverpage_news[i].find(regex_h).get_text() \n",
    "        titles.append(t.strip())\n",
    "    except:\n",
    "        titles.append(\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay okay new method now we're gonna go with newspaper3k and its gonna be 3k times faster than soup\n",
    "question mark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvpinfo_swiat = newspaper.build(\"https://www.tvp.info/191867/swiat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in tvpinfo_swiat.articles:\n",
    "    #print(article.url)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for category in tvpinfo_swiat.category_urls():\n",
    "#     cat_paper = newspaper.build(category)\n",
    "#     #print(cat_paper.articles) #Gives all articles of category\n",
    "#     pass\n",
    "#     for article in cat_paper.articles:\n",
    "#         #print (article.url) #prints URL for all articles in given category\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New idea, how about Soup and newspaper together\n",
    "first soup up the titles and urls and use newspaper for text etcetera, maybe even summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = requests.get(\"https://www.tvp.info/191867/swiat\").text\n",
    "soup1 = BeautifulSoup(r1, 'html.parser')\n",
    "# regex = re.compile('.*main-mesh-box*')\n",
    "# coverpage_news = soup1.find_all(\"h3\", {\"class\" : regex})\n",
    "\n",
    "# coverpage_news\n",
    "soup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"https://www.tvp.info\"\n",
    "page_to_scrape = \"https://www.tvp.info/191867/swiat\"\n",
    "\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\Jedrek\\Documents\\scraping_NLP\\chromedriver.exe\")\n",
    "browser.get(\"https://www.tvp.info/191867/swiat\")\n",
    "source_code = (browser.page_source)\n",
    "\n",
    "# print(selected.get_property('attributes')[0])\n",
    "#print(selected.textContent)\n",
    "browser.quit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverpage = source_code\n",
    "soup1 = BeautifulSoup(coverpage, 'html.parser')\n",
    "regex = re.compile('.*main-mesh-box__text-container*')\n",
    "coverpage_news = soup1.find_all(\"div\", {\"class\" : regex})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverpage_news[1]\n",
    "# desc_news = soup1.find_all(\"div\", {\"class\" : 'main-mesh-box__description'})\n",
    "# [i.get_text().strip() for i in desc_news]\n",
    "location_news = soup1.find_all(\"span\", {\"class\" : 'time-article__category'})\n",
    "[i.get_text() for i in location_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverpage_news[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "regex_h = re.compile('[h1|h2|h3]')\n",
    "for i in range(len(coverpage_news)):\n",
    "    try:\n",
    "        t = coverpage_news[i].find(regex_h).get_text() \n",
    "        titles.append(t.strip())\n",
    "    except:\n",
    "        titles.append(\"NA\")\n",
    "\n",
    "urls = [f\"{domain}{coverpage_news[i].find('a')['href']}\" for i in range(len(coverpage_news))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's try to do the boogie in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_to_scrape_world = [f\"https://www.tvp.info/191867/swiat?page={i}\" for i in range(1,10)]\n",
    "pages_to_scrape_poland = [f\"https://www.tvp.info/191866/polska?page={i}\" for i in range(1,10)]\n",
    "pages_to_scrape = pages_to_scrape_world + pages_to_scrape_poland\n",
    "domain = \"https://www.tvp.info\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "titles_all = []\n",
    "urls_all = []\n",
    "locations_all = []\n",
    "pages_all = []\n",
    "dates_all = []\n",
    "descriptions_all = []\n",
    "for page in pages_to_scrape:\n",
    "    # open the page with Selenium to get the js to render (requests don't work)\n",
    "    browser = webdriver.Chrome(executable_path=r\"C:\\Users\\Jedrek\\Documents\\scraping_NLP\\chromedriver.exe\")\n",
    "    browser.get(page)\n",
    "    source_code = (browser.page_source)\n",
    "    browser.quit()\n",
    "    # soup it up\n",
    "    coverpage = source_code\n",
    "    soup1 = BeautifulSoup(coverpage, 'html.parser')\n",
    "    regex = re.compile('.*main-mesh-box__text-container*')\n",
    "    coverpage_news = soup1.find_all(\"div\", {\"class\" : regex})\n",
    "    dates_news = soup1.find_all(\"span\", {\"class\" : 'time-article__time'})\n",
    "    desc_news = soup1.find_all(\"div\", {\"class\" : 'main-mesh-box__description'})\n",
    "    location_news = soup1.find_all(\"span\", {\"class\" : 'time-article__category'})\n",
    "    locations = [i.get_text() for i in location_news]\n",
    "    dates = [i.get_text() for i in dates_news]\n",
    "    descriptions = [i.get_text().strip() for i in desc_news]\n",
    "    # urls and titles \n",
    "    regex_h = re.compile('[h1|h2|h3]')\n",
    "    titles = []\n",
    "    for i in range(len(coverpage_news)):\n",
    "        try:\n",
    "            t = coverpage_news[i].find(regex_h).get_text() \n",
    "            titles.append(t.strip())\n",
    "        except:\n",
    "            titles.append(\"NA\")\n",
    "    pages = [page[-1] for i in titles] \n",
    "    urls = [f\"{domain}{coverpage_news[i].find('a')['href']}\" for i in range(len(coverpage_news))]\n",
    "    titles_all = titles_all + titles\n",
    "    urls_all = urls_all + urls\n",
    "    locations_all = locations_all + locations\n",
    "    pages_all = pages_all + pages\n",
    "    dates_all = dates_all + dates\n",
    "    descriptions_all = descriptions_all + descriptions\n",
    "elapsed_time = time.process_time() - start\n",
    "elapsed_time\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine results into a big df\n",
    "column_names = ['Location', 'Page_n', 'Title', 'Url', 'Date', 'Description']\n",
    "df = pd.DataFrame(list(zip(locations_all, pages_all, titles_all, urls_all, dates_all, descriptions_all)), columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out corona related news SUPER-naively\n",
    "df_corona = df[df[\"Description\"].str.contains(\"korona|szczepi|wirus\")|df[\"Title\"].str.contains(\"korona|szczepi|wirus\")]\n",
    "len(df_corona)\n",
    "# df_corona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = df_corona['Url'].iloc(0)[0]\n",
    "a = Article(url, language='pl')\n",
    "a.download()\n",
    "a.parse()\n",
    "list_clean = [i for i in (a.text).split(\"\\n\\n\") if \"Czytaj także\" not in i and \"wieszwiecej\" not in i and \"zobacz więcej\" not in i and \"źródło\" not in i]\n",
    "# print(a.title)\n",
    "text_clean = ' '.join(list_clean).strip()\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
    "# res = tokenizer.tokenize(text_clean)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.publish_date\n",
    "# a.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Weterynarz z Lodi na północy Włoch otrzymał niezwykłą misję do wykonania. Po ostrej fazie pandemii koronawirusa poleci do Kolumbii, by zająć się stadem hipopotamów, które należało do nieżyjącego od prawie 30 lat barona narkotykowego Pablo Escobara.' \n",
    "- this description was qualified by the naivete solution as a corona related article, which it really maybe isn't so that's something to look into\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.nlp()\n",
    "# a.summary\n",
    "# [i for i in df_corona['Url'][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = []\n",
    "for url in df_corona['Url']:\n",
    "    try:\n",
    "        a = Article(url, language='pl')\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        list_clean = [i for i in (a.text).split(\"\\n\\n\") if  \"wieszwiecej\" not in i and \"zobacz więcej\" not in i and \"źródło\" not in i and \"Zobacz także\" not in i and \"KORONAWIRUS – RAPORT\" not in i]\n",
    "        text_clean = ' '.join(list_clean).strip()\n",
    "        clean_texts.append(text_clean)\n",
    "    except:\n",
    "        clean_texts.append(\"ERROR\")\n",
    "\n",
    "df_corona['Clean_text'] = pd.Series(clean_texts).values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEGACY : \"Czytaj także\" not in i and, that seemed ot cut some articles last sentences \n",
    "\n",
    "Problem with some text being shaved for unidentified reasons: https://www.tvp.info/53412598/koronawirus-w-polsce-nowe-przypadki-zakazenia-covid-19-dane-ministerstwa-zdrowia-z-dzisiaj-21-kwietnia-sroda-rekord-zakazen-statystyki-liczba-zgonow-21042021\n",
    "\n",
    "Problem with embedded tweets that are not formatted well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in df_corona['Clean_text']:\n",
    "    print(i)\n",
    "    print(counter)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKAY NOW I WILL ATTEMPT TO SCRAPE EVERY ARTICLE FROM THIS YEAR\n",
    "ok until the end of february so like two months, 4000 articles, out of which 900 touched covid or vaccines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_to_scrape_world = [f\"https://www.tvp.info/191867/swiat?page={i}\" for i in range(1,100)]\n",
    "pages_to_scrape_poland = [f\"https://www.tvp.info/191866/polska?page={i}\" for i in range(1,100)]\n",
    "pages_to_scrape = pages_to_scrape_world + pages_to_scrape_poland\n",
    "domain = \"https://www.tvp.info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "titles_all = []\n",
    "urls_all = []\n",
    "locations_all = []\n",
    "pages_all = []\n",
    "dates_all = []\n",
    "descriptions_all = []\n",
    "for page in pages_to_scrape:\n",
    "    # open the page with Selenium to get the js to render (requests don't work)\n",
    "    browser = webdriver.Chrome(executable_path=r\"C:\\Users\\Jedrek\\Documents\\scraping_NLP\\chromedriver.exe\")\n",
    "    browser.get(page)\n",
    "    source_code = (browser.page_source)\n",
    "    browser.quit()\n",
    "    # soup it up\n",
    "    coverpage = source_code\n",
    "    soup1 = BeautifulSoup(coverpage, 'html.parser')\n",
    "    regex = re.compile('.*main-mesh-box__text-container*')\n",
    "    coverpage_news = soup1.find_all(\"div\", {\"class\" : regex})\n",
    "    dates_news = soup1.find_all(\"span\", {\"class\" : 'time-article__time'})\n",
    "    desc_news = soup1.find_all(\"div\", {\"class\" : 'main-mesh-box__description'})\n",
    "    location_news = soup1.find_all(\"span\", {\"class\" : 'time-article__category'})\n",
    "    locations = [i.get_text() for i in location_news]\n",
    "    dates = [i.get_text() for i in dates_news]\n",
    "    descriptions = [i.get_text().strip() for i in desc_news]\n",
    "    # urls and titles \n",
    "    regex_h = re.compile('[h1|h2|h3]')\n",
    "    titles = []\n",
    "    for i in range(len(coverpage_news)):\n",
    "        try:\n",
    "            t = coverpage_news[i].find(regex_h).get_text() \n",
    "            titles.append(t.strip())\n",
    "        except:\n",
    "            titles.append(\"NA\")\n",
    "    pages = [page[-1] for i in titles] \n",
    "    urls = [f\"{domain}{coverpage_news[i].find('a')['href']}\" for i in range(len(coverpage_news))]\n",
    "    titles_all = titles_all + titles\n",
    "    urls_all = urls_all + urls\n",
    "    locations_all = locations_all + locations\n",
    "    pages_all = pages_all + pages\n",
    "    dates_all = dates_all + dates\n",
    "    descriptions_all = descriptions_all + descriptions\n",
    "\n",
    "\n",
    "column_names = ['Location', 'Page_n', 'Title', 'Url', 'Date', 'Description']\n",
    "df = pd.DataFrame(list(zip(locations_all, pages_all, titles_all, urls_all, dates_all, descriptions_all)), columns = column_names)\n",
    "df_corona = df[df[\"Description\"].str.contains(\"korona|szczepi|wirus\")|df[\"Title\"].str.contains(\"korona|szczepi|wirus\")]\n",
    "\n",
    "clean_texts = []\n",
    "for url in df_corona['Url']:\n",
    "    try:\n",
    "        a = Article(url, language='pl')\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        list_clean = [i for i in (a.text).split(\"\\n\\n\") if  \"wieszwiecej\" not in i and \"zobacz więcej\" not in i and \"źródło\" not in i and \"Zobacz także\" not in i and \"KORONAWIRUS – RAPORT\" not in i]\n",
    "        text_clean = ' '.join(list_clean).strip()\n",
    "        clean_texts.append(text_clean)\n",
    "    except:\n",
    "        clean_texts.append(\"ERROR\")\n",
    "\n",
    "df_corona['Clean_text'] = pd.Series(clean_texts).values\n",
    "\n",
    "elapsed_time = time.process_time() - start\n",
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-342-2d8c24d72fc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mclean_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ERROR\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf_corona_title\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Clean_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2938\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3000\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3635\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3636\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3638\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index, copy)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Length of values does not match length of index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "df_corona_title = df[df[\"Title\"].str.contains(\"korona|szczepi|wirus\")] # just title containing\n",
    "\n",
    "for url in df_corona_title['Url']:\n",
    "    try:\n",
    "        a = Article(url, language='pl')\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        list_clean = [i for i in (a.text).split(\"\\n\\n\") if  \"wieszwiecej\" not in i and \"zobacz więcej\" not in i and \"źródło\" not in i and \"Zobacz także\" not in i and \"KORONAWIRUS – RAPORT\" not in i]\n",
    "        text_clean = ' '.join(list_clean).strip()\n",
    "        clean_texts.append(text_clean)\n",
    "    except:\n",
    "        clean_texts.append(\"ERROR\")\n",
    "\n",
    "df_corona_title['Clean_text'] = pd.Series(clean_texts).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_corona_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona.to_csv(\"df_corona.csv\") # looks shitty, no special characters, and terrible rows \n",
    "\n",
    "df.to_excel(\"df_all.xlsx\")\n",
    "df_corona.to_excel(\"df_corona.xlsx\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
