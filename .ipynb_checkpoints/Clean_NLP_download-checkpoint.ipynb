{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from selenium import webdriver\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_to_scrape_world = [f\"https://www.tvp.info/191867/swiat?page={i}\" for i in range(1,300)]\n",
    "pages_to_scrape_poland = [f\"https://www.tvp.info/191866/polska?page={i}\" for i in range(1,500)]\n",
    "pages_to_scrape = pages_to_scrape_world + pages_to_scrape_poland\n",
    "domain = \"https://www.tvp.info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jedrek\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n",
      "<ipython-input-5-9acb98f95a3f>:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_corona['Clean_text'] = pd.Series(clean_texts).values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72.0625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "titles_all = []\n",
    "urls_all = []\n",
    "locations_all = []\n",
    "pages_all = []\n",
    "dates_all = []\n",
    "descriptions_all = []\n",
    "for page in pages_to_scrape:\n",
    "    # open the page with Selenium to get the js to render (requests don't work)\n",
    "    browser = webdriver.Chrome(executable_path=r\"C:\\Users\\Jedrek\\Documents\\scraping_NLP\\chromedriver.exe\")\n",
    "    browser.get(page)\n",
    "    source_code = (browser.page_source)\n",
    "    browser.quit()\n",
    "    # soup it up\n",
    "    coverpage = source_code\n",
    "    soup1 = BeautifulSoup(coverpage, 'html.parser')\n",
    "    regex = re.compile('.*main-mesh-box__text-container*')\n",
    "    coverpage_news = soup1.find_all(\"div\", {\"class\" : regex})\n",
    "    dates_news = soup1.find_all(\"span\", {\"class\" : 'time-article__time'})\n",
    "    desc_news = soup1.find_all(\"div\", {\"class\" : 'main-mesh-box__description'})\n",
    "    location_news = soup1.find_all(\"span\", {\"class\" : 'time-article__category'})\n",
    "    locations = [i.get_text() for i in location_news]\n",
    "    dates = [i.get_text() for i in dates_news]\n",
    "    descriptions = [i.get_text().strip() for i in desc_news]\n",
    "    # urls and titles \n",
    "    regex_h = re.compile('[h1|h2|h3]')\n",
    "    titles = []\n",
    "    for i in range(len(coverpage_news)):\n",
    "        try:\n",
    "            t = coverpage_news[i].find(regex_h).get_text() \n",
    "            titles.append(t.strip())\n",
    "        except:\n",
    "            titles.append(\"NA\")\n",
    "    pages = [page[-1] for i in titles] \n",
    "    urls = [f\"{domain}{coverpage_news[i].find('a')['href']}\" for i in range(len(coverpage_news))]\n",
    "    titles_all = titles_all + titles\n",
    "    urls_all = urls_all + urls\n",
    "    locations_all = locations_all + locations\n",
    "    pages_all = pages_all + pages\n",
    "    dates_all = dates_all + dates\n",
    "    descriptions_all = descriptions_all + descriptions\n",
    "\n",
    "pattern = \"(?i)(epidem\\w*)|(covid\\w*\\W*1?9?)|(pandem\\w*)|(koronaw*)|(zakaż\\w*\\W*)\"\n",
    "column_names = ['Location', 'Page_n', 'Title', 'Url', 'Date', 'Description']\n",
    "df = pd.DataFrame(list(zip(locations_all, pages_all, titles_all, urls_all, dates_all, descriptions_all)), columns = column_names)\n",
    "df_corona = df[df[\"Title\"].str.contains(pattern)]\n",
    "df.to_excel(\"df_full_20210523.xlsx\")\n",
    "clean_texts = []\n",
    "for url in df_corona['Url']:\n",
    "    try:\n",
    "        a = Article(url, language='pl')\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        list_clean = [i for i in (a.text).split(\"\\n\\n\") if  \"wieszwiecej\" not in i and \"zobacz więcej\" not in i and \"źródło\" not in i and \"KORONAWIRUS – RAPORT\" not in i]\n",
    "        text_clean = ' '.join(list_clean).strip()\n",
    "        clean_texts.append(text_clean)\n",
    "    except:\n",
    "        clean_texts.append(\"ERROR\")\n",
    "\n",
    "df_corona['Clean_text'] = pd.Series(clean_texts).values\n",
    "df_corona.to_excel(\"df_corona_20210523.xlsx\")\n",
    "\n",
    "\n",
    "elapsed_time = time.process_time() - start\n",
    "elapsed_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
